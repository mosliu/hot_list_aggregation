"""
äº‹ä»¶åˆå¹¶æœåŠ¡ï¼ˆæ‰¹é‡åˆ†æç‰ˆï¼‰
è´Ÿè´£è¯†åˆ«å’Œåˆå¹¶ç›¸ä¼¼çš„çƒ­ç‚¹äº‹ä»¶ï¼Œä½¿ç”¨æ‰¹é‡LLMåˆ†ææ›¿ä»£é€å¯¹æ¯”è¾ƒ
"""

import asyncio
import json
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from loguru import logger
from sqlalchemy.orm import Session
from sqlalchemy import and_, desc, or_

from database.connection import get_db_session
from models.hot_aggr_models import (
    HotAggrEvent,
    HotAggrNewsEventRelation,
    HotAggrEventHistoryRelation
)
from services.llm_wrapper import llm_wrapper
from services.prompt_templates import prompt_templates
from config.settings import settings


class EventCombineService:
    """äº‹ä»¶åˆå¹¶æœåŠ¡ç±»ï¼ˆæ‰¹é‡åˆ†æç‰ˆï¼‰"""

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        self.combine_count = getattr(settings, 'EVENT_COMBINE_COUNT', 30)
        self.confidence_threshold = getattr(settings, 'EVENT_COMBINE_CONFIDENCE_THRESHOLD', 0.75)

    async def get_recent_events(self, count: int = None) -> List[Dict]:
        """
        è·å–æœ€è¿‘çš„äº‹ä»¶åˆ—è¡¨

        Args:
            count: è·å–çš„äº‹ä»¶æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼

        Returns:
            äº‹ä»¶åˆ—è¡¨
        """
        if count is None:
            count = self.combine_count

        try:
            with get_db_session() as db:
                events = db.query(HotAggrEvent).filter(
                    HotAggrEvent.status == 1  # åªè·å–æ­£å¸¸çŠ¶æ€çš„äº‹ä»¶
                ).order_by(
                    desc(HotAggrEvent.created_at)
                ).limit(count).all()

                event_list = []
                for event in events:
                    event_list.append({
                        'id': event.id,
                        'title': event.title or '',
                        'description': event.description or '',
                        'event_type': event.event_type or '',
                        'sentiment': event.sentiment or '',
                        'entities': event.entities or '',
                        'regions': event.regions or '',
                        'keywords': event.keywords or '',
                        'confidence_score': float(event.confidence_score or 0),
                        'news_count': event.news_count or 0,
                        'first_news_time': event.first_news_time,
                        'last_news_time': event.last_news_time,
                        'created_at': event.created_at,
                        'updated_at': event.updated_at
                    })

                logger.info(f"è·å–åˆ° {len(event_list)} ä¸ªæœ€è¿‘äº‹ä»¶")
                return event_list

        except Exception as e:
            logger.error(f"è·å–æœ€è¿‘äº‹ä»¶å¤±è´¥: {e}")
            raise

    async def get_events_by_ids(self, event_ids: List[int]) -> List[Dict]:
        """
        æ ¹æ®äº‹ä»¶IDåˆ—è¡¨è·å–äº‹ä»¶ä¿¡æ¯

        Args:
            event_ids: äº‹ä»¶IDåˆ—è¡¨

        Returns:
            äº‹ä»¶åˆ—è¡¨
        """
        try:
            with get_db_session() as db:
                events = db.query(HotAggrEvent).filter(
                    and_(
                        HotAggrEvent.id.in_(event_ids),
                        HotAggrEvent.status == 1  # åªè·å–æ­£å¸¸çŠ¶æ€çš„äº‹ä»¶
                    )
                ).all()

                event_list = []
                for event in events:
                    event_list.append({
                        'id': event.id,
                        'title': event.title or '',
                        'description': event.description or '',
                        'event_type': event.event_type or '',
                        'sentiment': event.sentiment or '',
                        'entities': event.entities or '',
                        'regions': event.regions or '',
                        'keywords': event.keywords or '',
                        'confidence_score': float(event.confidence_score or 0),
                        'news_count': event.news_count or 0,
                        'first_news_time': event.first_news_time,
                        'last_news_time': event.last_news_time,
                        'created_at': event.created_at,
                        'updated_at': event.updated_at
                    })

                logger.info(f"æ ¹æ®IDè·å–åˆ° {len(event_list)} ä¸ªäº‹ä»¶ï¼Œè¯·æ±‚ID: {event_ids}")
                return event_list

        except Exception as e:
            logger.error(f"æ ¹æ®IDè·å–äº‹ä»¶å¤±è´¥: {e}")
            raise

    def _format_events_for_batch_analysis(self, events: List[Dict]) -> str:
        """
        å°†äº‹ä»¶åˆ—è¡¨æ ¼å¼åŒ–ä¸ºæ‰¹é‡åˆ†æçš„JSONå­—ç¬¦ä¸²

        Args:
            events: äº‹ä»¶åˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–åçš„äº‹ä»¶JSONå­—ç¬¦ä¸²
        """
        try:
            formatted_events = []

            for event in events:
                formatted_event = {
                    'id': event['id'],
                    'title': event.get('title', ''),
                    'description': event.get('description', ''),
                    'event_type': event.get('event_type', ''),
                    'sentiment': event.get('sentiment', ''),
                    'entities': event.get('entities', ''),
                    'regions': event.get('regions', ''),
                    'keywords': event.get('keywords', ''),
                    'confidence_score': event.get('confidence_score', 0),
                    'news_count': event.get('news_count', 0),
                    'first_news_time': event.get('first_news_time').strftime('%Y-%m-%d %H:%M:%S') if event.get('first_news_time') else '',
                    'last_news_time': event.get('last_news_time').strftime('%Y-%m-%d %H:%M:%S') if event.get('last_news_time') else '',
                    'created_at': event.get('created_at').strftime('%Y-%m-%d %H:%M:%S') if event.get('created_at') else ''
                }
                formatted_events.append(formatted_event)

            # æ ¼å¼åŒ–ä¸ºå¯è¯»çš„JSONå­—ç¬¦ä¸²
            events_json = json.dumps(formatted_events, ensure_ascii=False, indent=2)
            return events_json

        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–äº‹ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return "[]"  # è¿”å›ç©ºæ•°ç»„ä½œä¸ºfallback

    async def analyze_events_batch_merge(self, events: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨LLMæ‰¹é‡åˆ†æäº‹ä»¶åˆ—è¡¨ï¼Œè¯†åˆ«å‡ºåº”è¯¥åˆå¹¶çš„äº‹ä»¶ç»„

        Args:
            events: äº‹ä»¶åˆ—è¡¨

        Returns:
            åˆå¹¶ç»„åˆ—è¡¨ï¼Œæ¯ç»„åŒ…å«å¤šä¸ªè¦åˆå¹¶çš„äº‹ä»¶
        """
        try:
            if len(events) < 2:
                logger.info("äº‹ä»¶æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡åˆ†æ")
                return []

            logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡äº‹ä»¶åˆ†æ: {len(events)} ä¸ªäº‹ä»¶")

            # æ ¼å¼åŒ–äº‹ä»¶åˆ—è¡¨ä¸ºJSON
            events_json = self._format_events_for_batch_analysis(events)
            logger.debug(f"äº‹ä»¶æ•°æ®JSONå¤§å°: {len(events_json)} å­—ç¬¦")

            # ä½¿ç”¨æ–°çš„æ‰¹é‡åˆ†ææ¨¡æ¿
            prompt = prompt_templates.format_template(
                'event_batch_merge_analysis',
                events_list=events_json
            )

            # è°ƒç”¨LLMè¿›è¡Œæ‰¹é‡åˆ†æ
            model_name = getattr(settings, 'EVENT_COMBINE_MODEL', 'gemini-2.5-pro')
            temperature = getattr(settings, 'EVENT_COMBINE_TEMPERATURE', 0.7)
            max_tokens = getattr(settings, 'EVENT_COMBINE_MAX_TOKENS', 600000)  # æ‰¹é‡åˆ†æéœ€è¦æ›´å¤šä»¤ç‰Œ

            # è®°å½•LLMè°ƒç”¨å¼€å§‹
            logger.info(f"ğŸ§  LLMæ‰¹é‡åˆ†æè°ƒç”¨: åˆ†æ {len(events)} ä¸ªäº‹ä»¶")
            logger.info(f"  æ¨¡å‹: {model_name}")
            logger.info(f"  æ¸©åº¦: {temperature}")
            logger.info(f"  æœ€å¤§ä»¤ç‰Œ: {max_tokens}")
            logger.info(f"  Promptå¤§å°: {len(prompt)} å­—ç¬¦")

            call_start_time = datetime.now()
            response_text = None

            # é‡è¯•æœºåˆ¶
            max_retries = getattr(settings, 'EVENT_COMBINE_RETRY_TIMES', 3)
            for retry in range(max_retries):
                try:
                    retry_start_time = datetime.now()
                    logger.info(f"  ğŸ”„ å°è¯• {retry + 1}/{max_retries}")

                    response_text = await llm_wrapper.call_llm_single(
                        prompt=prompt,
                        model=model_name,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )

                    retry_duration = (datetime.now() - retry_start_time).total_seconds()

                    if response_text:
                        logger.info(f"  âœ… æ‰¹é‡åˆ†ææˆåŠŸï¼Œè€—æ—¶: {retry_duration:.2f}ç§’")
                        logger.info(f"  å“åº”å¤§å°: {len(response_text)} å­—ç¬¦")
                        break
                    else:
                        logger.warning(f"  âš ï¸ å“åº”ä¸ºç©ºï¼Œè€—æ—¶: {retry_duration:.2f}ç§’")

                except Exception as retry_error:
                    retry_duration = (datetime.now() - retry_start_time).total_seconds()
                    if retry == max_retries - 1:
                        logger.error(f"  âŒ æœ€ç»ˆå¤±è´¥ï¼Œè€—æ—¶: {retry_duration:.2f}ç§’, é”™è¯¯: {retry_error}")
                        raise retry_error
                    logger.warning(f"  ğŸ”„ é‡è¯• {retry + 1}/{max_retries}ï¼Œè€—æ—¶: {retry_duration:.2f}ç§’, é”™è¯¯: {retry_error}")
                    await asyncio.sleep(2)  # æ‰¹é‡åˆ†æç­‰å¾…æ›´ä¹…

            total_duration = (datetime.now() - call_start_time).total_seconds()
            logger.info(f"ğŸ“Š LLMæ‰¹é‡åˆ†æå®Œæˆï¼Œæ€»è€—æ—¶: {total_duration:.2f}ç§’")

            # è§£æJSONå“åº”
            if not response_text:
                logger.error("æ‰¹é‡åˆ†æLLMå“åº”ä¸ºç©º")
                return []

            try:
                logger.info(f"  ğŸ”§ å¼€å§‹è§£ææ‰¹é‡åˆ†æJSONå“åº”...")
                response = json.loads(response_text)
                logger.info(f"  âœ… JSONè§£ææˆåŠŸ")
            except json.JSONDecodeError as json_error:
                logger.warning(f"  âš ï¸ JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {json_error}")
                try:
                    import json_repair
                    response = json_repair.loads(response_text)
                    logger.info(f"  ğŸ”§ JSONä¿®å¤æˆåŠŸ")
                except Exception as repair_error:
                    logger.error(f"  âŒ JSONä¿®å¤å¤±è´¥: {repair_error}")
                    logger.debug(f"  åŸå§‹å“åº”å‰500å­—ç¬¦: {response_text[:500]}...")
                    return []

            # å¤„ç†æ‰¹é‡åˆ†æç»“æœ
            merge_suggestions = []
            if 'merge_suggestions' in response:
                raw_suggestions = response['merge_suggestions']
                logger.info(f"è·å¾— {len(raw_suggestions)} ä¸ªåŸå§‹åˆå¹¶å»ºè®®")

                for suggestion in raw_suggestions:
                    try:
                        # éªŒè¯åˆå¹¶å»ºè®®çš„åŸºæœ¬å­—æ®µ
                        if not all(field in suggestion for field in ['events_to_merge', 'primary_event_id', 'confidence']):
                            logger.warning(f"è·³è¿‡ä¸å®Œæ•´çš„åˆå¹¶å»ºè®®: {suggestion}")
                            continue

                        # æ£€æŸ¥ç½®ä¿¡åº¦é˜ˆå€¼
                        confidence = suggestion.get('confidence', 0)
                        if confidence < self.confidence_threshold:
                            logger.debug(f"è·³è¿‡ä½ç½®ä¿¡åº¦åº»è®®: {confidence:.3f} < {self.confidence_threshold}")
                            continue

                        # æ£€æŸ¥äº‹ä»¶IDçš„æœ‰æ•ˆæ€§
                        events_to_merge = suggestion['events_to_merge']
                        if len(events_to_merge) < 2:
                            logger.warning(f"è·³è¿‡äº‹ä»¶æ•°é‡ä¸è¶³çš„åˆå¹¶å»ºè®®: {events_to_merge}")
                            continue

                        # éªŒè¯äº‹ä»¶IDå­˜åœ¨
                        event_ids = [event['id'] for event in events]
                        valid_event_ids = [eid for eid in events_to_merge if eid in event_ids]
                        if len(valid_event_ids) != len(events_to_merge):
                            logger.warning(f"è·³è¿‡åŒ…å«æ— æ•ˆäº‹ä»¶IDçš„å»ºè®®: {events_to_merge}")
                            continue

                        # è·å–ç›¸å…³äº‹ä»¶å¯¹è±¡
                        merge_events = [event for event in events if event['id'] in events_to_merge]
                        primary_event = next((event for event in merge_events if event['id'] == suggestion['primary_event_id']), merge_events[0])

                        # æ„å»ºæ‰¹é‡åˆå¹¶å»ºè®®
                        batch_merge_suggestion = {
                            'group_id': suggestion.get('group_id', f"batch_merge_{len(merge_suggestions) + 1}"),
                            'events_to_merge': events_to_merge,
                            'primary_event_id': suggestion['primary_event_id'],
                            'primary_event': primary_event,
                            'merge_events': merge_events,
                            'confidence': confidence,
                            'reason': suggestion.get('reason', ''),
                            'merged_title': suggestion.get('merged_title', primary_event['title']),
                            'merged_description': suggestion.get('merged_description', primary_event['description']),
                            'merged_keywords': suggestion.get('merged_keywords', primary_event.get('keywords', '')),
                            'merged_regions': suggestion.get('merged_regions', primary_event.get('regions', '')),
                            'analysis': suggestion.get('analysis', {})
                        }
                        merge_suggestions.append(batch_merge_suggestion)

                        logger.info(f"âœ… æœ‰æ•ˆæ‰¹é‡åˆå¹¶å»ºè®®: äº‹ä»¶{events_to_merge} -> ä¸»äº‹ä»¶{suggestion['primary_event_id']}, "
                                  f"ç½®ä¿¡åº¦: {confidence:.3f}")

                    except Exception as suggestion_error:
                        logger.error(f"å¤„ç†åˆå¹¶åº»è®®å¤±è´¥: {suggestion_error}")
                        continue

            # æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
            merge_suggestions.sort(key=lambda x: x['confidence'], reverse=True)

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            analysis_summary = response.get('analysis_summary', {})
            logger.info(f"ğŸ“Š æ‰¹é‡åˆ†æç»Ÿè®¡:")
            logger.info(f"  è¾“å…¥äº‹ä»¶æ•°: {len(events)}")
            logger.info(f"  LLMè°ƒç”¨æ¬¡æ•°: 1 (æ‰¹é‡åˆ†æ)")
            logger.info(f"  æœ‰æ•ˆåˆå¹¶ç»„: {len(merge_suggestions)}")
            logger.info(f"  æ€»è€—æ—¶: {total_duration:.2f}ç§’")

            if merge_suggestions:
                logger.info("åˆå¹¶ç»„è¯¦æƒ…:")
                for i, suggestion in enumerate(merge_suggestions, 1):
                    logger.info(f"  ç»„ {i}: {suggestion['events_to_merge']} -> {suggestion['primary_event_id']}, "
                              f"ç½®ä¿¡åº¦: {suggestion['confidence']:.3f}")

            return merge_suggestions

        except Exception as e:
            logger.error(f"æ‰¹é‡äº‹ä»¶åˆ†æå¤±è´¥: {e}")
            raise

    def _merge_multiple_events_data(self, merge_events: List[Dict], primary_event: Dict,
                                   merged_title: str, merged_description: str,
                                   merged_keywords: str, merged_regions: str) -> Dict:
        """
        èåˆå¤šä¸ªäº‹ä»¶çš„æ•°æ®

        Args:
            merge_events: è¦åˆå¹¶çš„äº‹ä»¶åˆ—è¡¨
            primary_event: ä¸»äº‹ä»¶ï¼ˆä¿ç•™ï¼‰
            merged_title: åˆå¹¶åçš„æ ‡é¢˜
            merged_description: åˆå¹¶åçš„æè¿°
            merged_keywords: åˆå¹¶åçš„å…³é”®è¯
            merged_regions: åˆå¹¶åçš„åœ°åŒº

        Returns:
            èåˆåçš„äº‹ä»¶æ•°æ®
        """
        try:
            # åˆå¹¶regions
            all_regions = set()
            for event in merge_events:
                if event.get('regions'):
                    regions_str = event['regions']
                    if regions_str.startswith('['):
                        try:
                            all_regions.update(json.loads(regions_str))
                        except:
                            all_regions.update([r.strip() for r in regions_str.split(',') if r.strip()])
                    else:
                        all_regions.update([r.strip() for r in regions_str.split(',') if r.strip()])

            # å¦‚æœLLMæä¾›äº†merged_regionsï¼Œä¼˜å…ˆä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨åˆå¹¶çš„regions
            final_regions = merged_regions if merged_regions else ','.join(sorted(all_regions))

            # åˆå¹¶keywords
            all_keywords = set()
            for event in merge_events:
                if event.get('keywords'):
                    all_keywords.update([k.strip() for k in event['keywords'].split(',') if k.strip()])

            # å¦‚æœLLMæä¾›äº†merged_keywordsï¼Œä¼˜å…ˆä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨åˆå¹¶çš„keywords
            final_keywords = merged_keywords if merged_keywords else ','.join(sorted(all_keywords))

            # åˆå¹¶entitiesï¼ˆå–æœ€è¯¦ç»†çš„ï¼‰
            all_entities = [event.get('entities', '') for event in merge_events if event.get('entities')]
            merged_entities = max(all_entities, key=len) if all_entities else primary_event.get('entities', '')

            # å–æœ€æ—©çš„é¦–æ¬¡æŠ¥é“æ—¶é—´å’Œæœ€æ™šçš„æœ€åæ›´æ–°æ—¶é—´
            first_news_times = [event.get('first_news_time') for event in merge_events if event.get('first_news_time')]
            last_news_times = [event.get('last_news_time') for event in merge_events if event.get('last_news_time')]

            first_news_time = min(first_news_times) if first_news_times else None
            last_news_time = max(last_news_times) if last_news_times else None

            # è®¡ç®—æ€»æ–°é—»æ•°é‡
            total_news_count = sum(event.get('news_count', 0) for event in merge_events)

            return {
                'title': merged_title or primary_event['title'],
                'description': merged_description or primary_event['description'],
                'event_type': primary_event.get('event_type'),
                'sentiment': primary_event.get('sentiment'),
                'entities': merged_entities,
                'regions': final_regions,
                'keywords': final_keywords,
                'first_news_time': first_news_time,
                'last_news_time': last_news_time,
                'news_count': total_news_count
            }

        except Exception as e:
            logger.error(f"èåˆå¤šä¸ªäº‹ä»¶æ•°æ®å¤±è´¥: {e}")
            return {
                'title': merged_title or primary_event['title'],
                'description': merged_description or primary_event['description'],
                'regions': primary_event.get('regions', ''),
                'keywords': primary_event.get('keywords', ''),
                'news_count': primary_event.get('news_count', 0)
            }

    def _create_manual_merge_suggestion(self, events: List[Dict],
                                      primary_event_id: int, primary_event: Dict) -> Dict:
        """
        åˆ›å»ºæ‰‹åŠ¨åˆå¹¶å»ºè®®

        Args:
            events: è¦åˆå¹¶çš„äº‹ä»¶åˆ—è¡¨
            primary_event_id: ä¸»äº‹ä»¶ID
            primary_event: ä¸»äº‹ä»¶æ•°æ®

        Returns:
            åˆå¹¶å»ºè®®
        """
        try:
            event_ids = [event['id'] for event in events]

            # åˆ›å»ºåˆå¹¶åçš„æ ‡é¢˜å’Œæè¿°
            titles = [event.get('title', '') for event in events if event.get('title')]
            descriptions = [event.get('description', '') for event in events if event.get('description')]

            # ä½¿ç”¨ä¸»äº‹ä»¶çš„æ ‡é¢˜ï¼Œæˆ–è€…åˆå¹¶æ‰€æœ‰æ ‡é¢˜
            merged_title = primary_event.get('title', '') or '; '.join(titles[:2])

            # ä½¿ç”¨ä¸»äº‹ä»¶çš„æè¿°ï¼Œæˆ–è€…åˆå¹¶æè¿°
            merged_description = primary_event.get('description', '') or '. '.join(descriptions[:2])

            # åˆå¹¶å…³é”®è¯
            all_keywords = set()
            for event in events:
                if event.get('keywords'):
                    keywords_str = event['keywords']
                    # å¤„ç†é€—å·åˆ†éš”çš„å…³é”®è¯
                    keywords = [k.strip() for k in keywords_str.split(',') if k.strip()]
                    all_keywords.update(keywords)
            merged_keywords = ','.join(sorted(all_keywords)[:10])  # é™åˆ¶å‰10ä¸ªå…³é”®è¯

            # åˆå¹¶åœ°åŒºä¿¡æ¯
            all_regions = set()
            for event in events:
                if event.get('regions'):
                    regions_str = event['regions']
                    if regions_str.startswith('['):
                        try:
                            regions = json.loads(regions_str)
                            all_regions.update(regions)
                        except:
                            regions = [r.strip() for r in regions_str.split(',') if r.strip()]
                            all_regions.update(regions)
                    else:
                        regions = [r.strip() for r in regions_str.split(',') if r.strip()]
                        all_regions.update(regions)
            merged_regions = ','.join(sorted(all_regions))

            # æ„å»ºæ‰‹åŠ¨åˆå¹¶å»ºè®®
            merge_suggestion = {
                'group_id': f'manual_merge_{primary_event_id}',
                'events_to_merge': event_ids,
                'primary_event_id': primary_event_id,
                'primary_event': primary_event,
                'merge_events': events,
                'confidence': 1.0,  # æ‰‹åŠ¨åˆå¹¶è®¾ç½®ä¸ºæœ€é«˜ç½®ä¿¡åº¦
                'reason': f'æ‰‹åŠ¨æŒ‡å®šåˆå¹¶äº‹ä»¶ {event_ids}ï¼Œä¸»äº‹ä»¶ {primary_event_id}',
                'merged_title': merged_title,
                'merged_description': merged_description,
                'merged_keywords': merged_keywords,
                'merged_regions': merged_regions,
                'analysis': {
                    'content_similarity': 1.0,  # æ‰‹åŠ¨åˆå¹¶å‡è®¾ä¸ºå®Œå…¨ç›¸å…³
                    'time_correlation': 1.0,
                    'location_correlation': 1.0,
                    'entity_correlation': 1.0
                }
            }

            logger.info(f"åˆ›å»ºæ‰‹åŠ¨åˆå¹¶å»ºè®®: äº‹ä»¶{event_ids} -> ä¸»äº‹ä»¶{primary_event_id}")
            logger.debug(f"åˆå¹¶åæ ‡é¢˜: {merged_title[:100]}...")
            logger.debug(f"åˆå¹¶åå…³é”®è¯: {merged_keywords}")

            return merge_suggestion

        except Exception as e:
            logger.error(f"åˆ›å»ºæ‰‹åŠ¨åˆå¹¶å»ºè®®å¤±è´¥: {e}")
            raise

    async def execute_batch_merge(self, merge_suggestion: Dict) -> bool:
        """
        æ‰§è¡Œæ‰¹é‡äº‹ä»¶åˆå¹¶

        Args:
            merge_suggestion: æ‰¹é‡åˆå¹¶å»ºè®®

        Returns:
            æ˜¯å¦åˆå¹¶æˆåŠŸ
        """
        events_to_merge = merge_suggestion['events_to_merge']
        primary_event_id = merge_suggestion['primary_event_id']

        logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œæ‰¹é‡åˆå¹¶: äº‹ä»¶{events_to_merge} -> ä¸»äº‹ä»¶{primary_event_id}")

        # è¯¦ç»†è®°å½•åˆå¹¶å»ºè®®ä¿¡æ¯
        logger.info(f"   åˆå¹¶å»ºè®®è¯¦æƒ…:")
        logger.info(f"     ç½®ä¿¡åº¦: {merge_suggestion.get('confidence', 'N/A')}")
        logger.info(f"     åˆå¹¶åŸå› : {merge_suggestion.get('reason', 'N/A')[:200]}...")
        logger.info(f"     ç›®æ ‡æ ‡é¢˜: {merge_suggestion.get('merged_title', 'N/A')[:100]}...")

        db_session = None
        merge_start_time = datetime.now()

        try:
            # 1. å»ºç«‹æ•°æ®åº“è¿æ¥å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
            logger.debug(f"  ğŸ”— æ­£åœ¨å»ºç«‹æ•°æ®åº“è¿æ¥...")
            try:
                db_session = get_db_session()
                db = db_session.__enter__()
                logger.info(f"  âœ… æ•°æ®åº“è¿æ¥å»ºç«‹æˆåŠŸ")
            except Exception as db_error:
                logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {db_error}")
                logger.error(f"   é”™è¯¯ç±»å‹: {type(db_error).__name__}")
                logger.exception("æ•°æ®åº“è¿æ¥è¯¦ç»†é”™è¯¯:")
                return False

            try:
                # 2. æŸ¥è¯¢ä¸»äº‹ä»¶ - å¢å¼ºé”™è¯¯å¤„ç†
                logger.debug(f"  ğŸ” æŸ¥è¯¢ä¸»äº‹ä»¶: {primary_event_id}")
                try:
                    primary_event = db.query(HotAggrEvent).filter(
                        HotAggrEvent.id == primary_event_id
                    ).first()

                    if not primary_event:
                        logger.error(f"âŒ ä¸»äº‹ä»¶æŸ¥è¯¢ç»“æœä¸ºç©º: ID={primary_event_id}")
                        # éªŒè¯æ•°æ®åº“ä¸­æ˜¯å¦çœŸçš„ä¸å­˜åœ¨è¯¥äº‹ä»¶
                        total_events = db.query(HotAggrEvent).count()
                        logger.error(f"   æ•°æ®åº“ä¸­æ€»äº‹ä»¶æ•°: {total_events}")
                        # æŸ¥è¯¢ç›¸è¿‘IDçš„äº‹ä»¶æ¥åˆ¤æ–­IDèŒƒå›´
                        nearby_events = db.query(HotAggrEvent.id).filter(
                            HotAggrEvent.id.between(primary_event_id - 5, primary_event_id + 5)
                        ).all()
                        logger.error(f"   ç›¸è¿‘IDäº‹ä»¶: {[e.id for e in nearby_events]}")
                        return False

                    logger.info(f"  âœ… ä¸»äº‹ä»¶æ‰¾åˆ°: ID={primary_event.id}, æ ‡é¢˜='{primary_event.title}', çŠ¶æ€={primary_event.status}")
                    logger.debug(f"     ä¸»äº‹ä»¶è¯¦æƒ…: æ–°é—»æ•°={primary_event.news_count}, åˆ›å»ºæ—¶é—´={primary_event.created_at}")

                except Exception as primary_query_error:
                    logger.error(f"âŒ æŸ¥è¯¢ä¸»äº‹ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {primary_query_error}")
                    logger.error(f"   SQLæŸ¥è¯¢å¤±è´¥: HotAggrEvent.id == {primary_event_id}")
                    logger.exception("ä¸»äº‹ä»¶æŸ¥è¯¢è¯¦ç»†é”™è¯¯:")
                    return False

                # 3. æŸ¥è¯¢æ‰€æœ‰è¦åˆå¹¶çš„äº‹ä»¶ - å¢å¼ºé”™è¯¯å¤„ç†
                logger.debug(f"  ğŸ” æŸ¥è¯¢åˆå¹¶äº‹ä»¶åˆ—è¡¨: {events_to_merge}")
                try:
                    merge_events = db.query(HotAggrEvent).filter(
                        HotAggrEvent.id.in_(events_to_merge)
                    ).all()

                    found_event_ids = [event.id for event in merge_events]
                    missing_event_ids = [eid for eid in events_to_merge if eid not in found_event_ids]

                    if len(merge_events) != len(events_to_merge):
                        logger.error(f"âŒ éƒ¨åˆ†åˆå¹¶äº‹ä»¶ä¸å­˜åœ¨:")
                        logger.error(f"     æœŸæœ›äº‹ä»¶: {events_to_merge}")
                        logger.error(f"     æ‰¾åˆ°äº‹ä»¶: {found_event_ids}")
                        logger.error(f"     ç¼ºå¤±äº‹ä»¶: {missing_event_ids}")

                        # å¯¹ç¼ºå¤±äº‹ä»¶è¿›è¡Œè¯¦ç»†è¯Šæ–­
                        for missing_id in missing_event_ids:
                            # æ£€æŸ¥æ˜¯å¦æ›¾ç»å­˜åœ¨ä½†è¢«åˆ é™¤
                            deleted_event = db.query(HotAggrEvent).filter(
                                and_(HotAggrEvent.id == missing_id, HotAggrEvent.status != 1)
                            ).first()
                            if deleted_event:
                                logger.error(f"     ç¼ºå¤±äº‹ä»¶ {missing_id} å­˜åœ¨ä½†çŠ¶æ€å¼‚å¸¸: status={deleted_event.status}")
                            else:
                                logger.error(f"     ç¼ºå¤±äº‹ä»¶ {missing_id} å®Œå…¨ä¸å­˜åœ¨äºæ•°æ®åº“ä¸­")

                        return False

                    logger.info(f"  âœ… æ‰€æœ‰åˆå¹¶äº‹ä»¶æ‰¾åˆ°: {found_event_ids}")

                    # è¯¦ç»†è®°å½•æ¯ä¸ªåˆå¹¶äº‹ä»¶çš„çŠ¶æ€
                    for event in merge_events:
                        logger.debug(f"     äº‹ä»¶{event.id}: æ ‡é¢˜='{event.title}', çŠ¶æ€={event.status}, æ–°é—»æ•°={event.news_count}")

                except Exception as merge_query_error:
                    logger.error(f"âŒ æŸ¥è¯¢åˆå¹¶äº‹ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸: {merge_query_error}")
                    logger.error(f"   SQL INæŸ¥è¯¢å¤±è´¥: HotAggrEvent.id.in_({events_to_merge})")
                    logger.exception("åˆå¹¶äº‹ä»¶æŸ¥è¯¢è¯¦ç»†é”™è¯¯:")
                    return False

                # 4. æ£€æŸ¥æ‰€æœ‰äº‹ä»¶çŠ¶æ€
                invalid_status_events = [(event.id, event.status) for event in merge_events if event.status != 1]
                if invalid_status_events:
                    logger.error(f"âŒ å­˜åœ¨éæ­£å¸¸çŠ¶æ€çš„äº‹ä»¶:")
                    for event_id, status in invalid_status_events:
                        logger.error(f"     äº‹ä»¶{event_id}: çŠ¶æ€={status} (æœŸæœ›çŠ¶æ€=1)")
                    logger.error(f"     åªèƒ½åˆå¹¶çŠ¶æ€ä¸º1çš„äº‹ä»¶")
                    return False

                logger.debug(f"  âœ… æ‰€æœ‰äº‹ä»¶çŠ¶æ€éªŒè¯é€šè¿‡")

                # 5. èåˆäº‹ä»¶æ•°æ® - å¢å¼ºé”™è¯¯å¤„ç†
                logger.debug(f"  ğŸ”„ å¼€å§‹èåˆäº‹ä»¶æ•°æ®")
                try:
                    logger.debug(f"     èåˆå‚æ•°éªŒè¯:")
                    logger.debug(f"       merge_eventsæ•°é‡: {len(merge_suggestion.get('merge_events', []))}")
                    logger.debug(f"       primary_eventå­˜åœ¨: {merge_suggestion.get('primary_event') is not None}")
                    logger.debug(f"       merged_titleé•¿åº¦: {len(merge_suggestion.get('merged_title', ''))}")

                    merged_data = self._merge_multiple_events_data(
                        merge_suggestion['merge_events'],
                        merge_suggestion['primary_event'],
                        merge_suggestion['merged_title'],
                        merge_suggestion['merged_description'],
                        merge_suggestion['merged_keywords'],
                        merge_suggestion['merged_regions']
                    )
                    logger.info(f"  âœ… äº‹ä»¶æ•°æ®èåˆå®Œæˆ")
                    logger.debug(f"     èåˆç»“æœ: æ ‡é¢˜é•¿åº¦={len(merged_data.get('title', ''))}, æ–°é—»æ•°={merged_data.get('news_count', 0)}")

                except Exception as merge_error:
                    logger.error(f"âŒ äº‹ä»¶æ•°æ®èåˆå¤±è´¥: {merge_error}")
                    logger.error(f"   èåˆå‡½æ•°: _merge_multiple_events_data")
                    logger.error(f"   è¾“å…¥å‚æ•°ç±»å‹æ£€æŸ¥:")
                    logger.error(f"     merge_events: {type(merge_suggestion.get('merge_events'))}")
                    logger.error(f"     primary_event: {type(merge_suggestion.get('primary_event'))}")
                    logger.exception("æ•°æ®èåˆè¯¦ç»†é”™è¯¯:")
                    return False

                # 6. æ›´æ–°ä¸»äº‹ä»¶ - å¢å¼ºé”™è¯¯å¤„ç†
                logger.debug(f"  ğŸ”„ æ›´æ–°ä¸»äº‹ä»¶æ•°æ®")
                try:
                    old_title = primary_event.title
                    old_news_count = primary_event.news_count

                    # é€å­—æ®µæ›´æ–°å¹¶è®°å½•
                    primary_event.title = merged_data['title']
                    primary_event.description = merged_data['description']
                    primary_event.event_type = merged_data.get('event_type')
                    primary_event.sentiment = merged_data.get('sentiment')
                    primary_event.entities = merged_data.get('entities')
                    primary_event.regions = merged_data['regions']
                    primary_event.keywords = merged_data['keywords']
                    primary_event.first_news_time = merged_data.get('first_news_time')
                    primary_event.last_news_time = merged_data.get('last_news_time')
                    primary_event.news_count = merged_data['news_count']
                    primary_event.updated_at = datetime.now()

                    logger.info(f"  âœ… ä¸»äº‹ä»¶æ›´æ–°å®Œæˆ:")
                    logger.info(f"     æ—§æ ‡é¢˜: '{old_title}'")
                    logger.info(f"     æ–°æ ‡é¢˜: '{merged_data['title']}'")
                    logger.info(f"     æ—§æ–°é—»æ•°: {old_news_count}")
                    logger.info(f"     æ–°æ–°é—»æ•°: {merged_data['news_count']}")

                except Exception as update_error:
                    logger.error(f"âŒ æ›´æ–°ä¸»äº‹ä»¶å¤±è´¥: {update_error}")
                    logger.error(f"   ä¸»äº‹ä»¶å¯¹è±¡: {primary_event}")
                    logger.error(f"   åˆå¹¶æ•°æ®: {merged_data}")
                    logger.exception("ä¸»äº‹ä»¶æ›´æ–°è¯¦ç»†é”™è¯¯:")
                    return False

                # 7. å¤„ç†æ¬¡è¦äº‹ä»¶å’Œæ–°é—»å…³è”è½¬ç§» - å¢å¼ºé”™è¯¯å¤„ç†
                secondary_events = [event for event in merge_events if event.id != primary_event_id]
                logger.info(f"  ğŸ”„ å¤„ç†æ¬¡è¦äº‹ä»¶: {[event.id for event in secondary_events]}")
                total_transferred_news = 0

                for secondary_event in secondary_events:
                    try:
                        logger.debug(f"    ğŸ”„ å¼€å§‹å¤„ç†æ¬¡è¦äº‹ä»¶ {secondary_event.id}")

                        # è®°å½•å¤„ç†å‰çŠ¶æ€
                        old_status = secondary_event.status
                        secondary_event.status = 2  # å·²åˆå¹¶çŠ¶æ€
                        secondary_event.updated_at = datetime.now()
                        logger.debug(f"       çŠ¶æ€æ›´æ–°: {old_status} -> 2")

                        # æŸ¥è¯¢æ–°é—»å…³è” - å¢å¼ºé”™è¯¯å¤„ç†
                        logger.debug(f"    ğŸ” æŸ¥è¯¢äº‹ä»¶ {secondary_event.id} çš„æ–°é—»å…³è”")
                        try:
                            news_relations = db.query(HotAggrNewsEventRelation).filter(
                                HotAggrNewsEventRelation.event_id == secondary_event.id
                            ).all()
                            logger.debug(f"    ğŸ“° æ‰¾åˆ° {len(news_relations)} ä¸ªæ–°é—»å…³è”")

                        except Exception as relation_query_error:
                            logger.error(f"âŒ æŸ¥è¯¢äº‹ä»¶{secondary_event.id}æ–°é—»å…³è”å¤±è´¥: {relation_query_error}")
                            logger.exception("æ–°é—»å…³è”æŸ¥è¯¢è¯¦ç»†é”™è¯¯:")
                            return False

                        transferred_news_count = 0
                        skipped_news_count = 0
                        relation_errors = []

                        for relation in news_relations:
                            try:
                                # æ£€æŸ¥ä¸»äº‹ä»¶æ˜¯å¦å·²ç»æœ‰è¿™æ¡æ–°é—»çš„å…³è”
                                existing_relation = db.query(HotAggrNewsEventRelation).filter(
                                    and_(
                                        HotAggrNewsEventRelation.news_id == relation.news_id,
                                        HotAggrNewsEventRelation.event_id == primary_event_id
                                    )
                                ).first()

                                if existing_relation:
                                    # åˆ é™¤é‡å¤å…³è”
                                    db.delete(relation)
                                    skipped_news_count += 1
                                    logger.debug(f"      â­ï¸ åˆ é™¤é‡å¤å…³è”: æ–°é—»{relation.news_id}")
                                else:
                                    # è½¬ç§»åˆ°ä¸»äº‹ä»¶
                                    old_event_id = relation.event_id
                                    relation.event_id = primary_event_id
                                    transferred_news_count += 1
                                    logger.debug(f"      â¡ï¸ è½¬ç§»æ–°é—»å…³è”: æ–°é—»{relation.news_id} ({old_event_id}->{primary_event_id})")

                            except Exception as relation_error:
                                relation_errors.append({
                                    'news_id': relation.news_id,
                                    'error': str(relation_error)
                                })
                                logger.error(f"      âŒ å¤„ç†æ–°é—»{relation.news_id}å…³è”å¤±è´¥: {relation_error}")

                        if relation_errors:
                            logger.error(f"    âŒ å¤„ç†æ–°é—»å…³è”æ—¶å‘ç”Ÿ {len(relation_errors)} ä¸ªé”™è¯¯")
                            for error_info in relation_errors:
                                logger.error(f"       æ–°é—»{error_info['news_id']}: {error_info['error']}")
                            return False

                        total_transferred_news += transferred_news_count
                        logger.info(f"    âœ… äº‹ä»¶{secondary_event.id}: è½¬ç§»{transferred_news_count}ä¸ªæ–°é—», è·³è¿‡{skipped_news_count}ä¸ªé‡å¤")

                    except Exception as secondary_error:
                        logger.error(f"âŒ å¤„ç†æ¬¡è¦äº‹ä»¶ {secondary_event.id} å¤±è´¥: {secondary_error}")
                        logger.error(f"   äº‹ä»¶å¯¹è±¡: {secondary_event}")
                        logger.exception("æ¬¡è¦äº‹ä»¶å¤„ç†è¯¦ç»†é”™è¯¯:")
                        return False

                # 8. è®°å½•åˆå¹¶å†å²å…³ç³» - å¢å¼ºé”™è¯¯å¤„ç†
                logger.debug(f"  ğŸ”„ è®°å½•åˆå¹¶å†å²å…³ç³»")
                try:
                    history_records = []
                    for secondary_event in secondary_events:
                        history_relation = HotAggrEventHistoryRelation(
                            parent_event_id=primary_event_id,
                            child_event_id=secondary_event.id,
                            relation_type='batch_merge',
                            confidence_score=merge_suggestion['confidence'],
                            description=f"æ‰¹é‡äº‹ä»¶åˆå¹¶: {merge_suggestion['reason']}",
                            created_at=datetime.now()
                        )
                        db.add(history_relation)
                        history_records.append(f"{secondary_event.id}->{primary_event_id}")
                        logger.debug(f"    ğŸ“ æ·»åŠ å†å²å…³ç³»: {secondary_event.id} -> {primary_event_id}")

                    logger.info(f"  âœ… åˆå¹¶å†å²è®°å½•å®Œæˆ: {len(history_records)} æ¡è®°å½•")

                except Exception as history_error:
                    logger.error(f"âŒ è®°å½•åˆå¹¶å†å²å¤±è´¥: {history_error}")
                    logger.error(f"   å†å²è®°å½•è¡¨: HotAggrEventHistoryRelation")
                    logger.error(f"   è¦è®°å½•çš„å…³ç³»æ•°é‡: {len(secondary_events)}")
                    logger.exception("å†å²è®°å½•è¯¦ç»†é”™è¯¯:")
                    return False

                # 9. æäº¤äº‹åŠ¡ - å¢å¼ºé”™è¯¯å¤„ç†
                logger.debug(f"  ğŸ’¾ æäº¤æ•°æ®åº“äº‹åŠ¡")
                commit_start_time = datetime.now()
                try:
                    db.commit()
                    commit_duration = (datetime.now() - commit_start_time).total_seconds()
                    logger.info(f"  âœ… æ•°æ®åº“äº‹åŠ¡æäº¤æˆåŠŸ (è€—æ—¶: {commit_duration:.3f}ç§’)")

                except Exception as commit_error:
                    commit_duration = (datetime.now() - commit_start_time).total_seconds()
                    logger.error(f"âŒ æ•°æ®åº“äº‹åŠ¡æäº¤å¤±è´¥ (å°è¯•æ—¶é•¿: {commit_duration:.3f}ç§’)")
                    logger.error(f"   æäº¤é”™è¯¯ç±»å‹: {type(commit_error).__name__}")
                    logger.error(f"   æäº¤é”™è¯¯ä¿¡æ¯: {commit_error}")
                    logger.exception("äº‹åŠ¡æäº¤è¯¦ç»†é”™è¯¯:")

                    # å°è¯•å›æ»š
                    rollback_start_time = datetime.now()
                    try:
                        db.rollback()
                        rollback_duration = (datetime.now() - rollback_start_time).total_seconds()
                        logger.info(f"  ğŸ”„ æ•°æ®åº“äº‹åŠ¡å›æ»šæˆåŠŸ (è€—æ—¶: {rollback_duration:.3f}ç§’)")
                    except Exception as rollback_error:
                        rollback_duration = (datetime.now() - rollback_start_time).total_seconds()
                        logger.error(f"âŒ æ•°æ®åº“å›æ»šå¤±è´¥ (å°è¯•æ—¶é•¿: {rollback_duration:.3f}ç§’): {rollback_error}")
                        logger.exception("å›æ»šè¯¦ç»†é”™è¯¯:")

                    return False

                # æˆåŠŸå®Œæˆ
                total_duration = (datetime.now() - merge_start_time).total_seconds()
                logger.info(f"ğŸ‰ æ‰¹é‡åˆå¹¶æˆåŠŸå®Œæˆ: {events_to_merge} -> ä¸»äº‹ä»¶{primary_event_id}")
                logger.info(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
                logger.info(f"   è½¬ç§»æ–°é—»å…³è”: {total_transferred_news} æ¡")
                logger.info(f"   ç½®ä¿¡åº¦: {merge_suggestion['confidence']:.3f}")
                logger.info(f"   åˆå¹¶åŸå› : {merge_suggestion['reason'][:100]}...")
                return True

            except Exception as inner_error:
                logger.error(f"âŒ åˆå¹¶è¿‡ç¨‹å†…éƒ¨é”™è¯¯: {inner_error}")
                logger.exception("åˆå¹¶è¿‡ç¨‹è¯¦ç»†é”™è¯¯:")

                # ç¡®ä¿å›æ»š
                try:
                    db.rollback()
                    logger.info(f"  ğŸ”„ æ•°æ®åº“å·²å›æ»š")
                except Exception as rollback_error:
                    logger.error(f"âŒ æ•°æ®åº“å›æ»šå¤±è´¥: {rollback_error}")

                return False

        except Exception as e:
            total_duration = (datetime.now() - merge_start_time).total_seconds()
            logger.error(f"âŒ æ‰§è¡Œæ‰¹é‡äº‹ä»¶åˆå¹¶å¤±è´¥ (æ€»è€—æ—¶: {total_duration:.2f}ç§’)")
            logger.error(f"   äº‹ä»¶åˆ—è¡¨: {events_to_merge}")
            logger.error(f"   ä¸»äº‹ä»¶ID: {primary_event_id}")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.exception("é¡¶å±‚é”™è¯¯è¯¦ç»†ä¿¡æ¯:")

            # æœ€ç»ˆå›æ»šä¿æŠ¤
            if db_session:
                try:
                    db = db_session.__enter__() if hasattr(db_session, '__enter__') else db_session
                    db.rollback()
                    logger.info(f"  ğŸ”„ æœ€ç»ˆä¿æŠ¤æ€§å›æ»šå®Œæˆ")
                except Exception as final_rollback_error:
                    logger.error(f"âŒ æœ€ç»ˆä¿æŠ¤æ€§å›æ»šå¤±è´¥: {final_rollback_error}")

            return False

        finally:
            # ç¡®ä¿æ•°æ®åº“è¿æ¥æ­£ç¡®å…³é—­
            if db_session:
                try:
                    if hasattr(db_session, '__exit__'):
                        db_session.__exit__(None, None, None)
                    else:
                        db_session.close()
                    logger.debug(f"  ğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")
                except Exception as close_error:
                    logger.error(f"âŒ å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {close_error}")

    async def run_manual_combine_process(self, event_ids: List[int]) -> Dict:
        """
        è¿è¡Œæ‰‹åŠ¨æŒ‡å®šäº‹ä»¶IDçš„åˆå¹¶æµç¨‹

        Args:
            event_ids: è¦åˆå¹¶çš„äº‹ä»¶IDåˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªIDå°†ä½œä¸ºä¸»äº‹ä»¶

        Returns:
            åˆå¹¶ç»“æœç»Ÿè®¡
        """
        start_time = datetime.now()

        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œæ‰‹åŠ¨äº‹ä»¶åˆå¹¶æµç¨‹ï¼Œäº‹ä»¶ID: {event_ids}")

            if len(event_ids) < 2:
                return {
                    'status': 'error',
                    'message': 'è‡³å°‘éœ€è¦2ä¸ªäº‹ä»¶IDè¿›è¡Œåˆå¹¶',
                    'total_events': len(event_ids),
                    'suggestions_count': 0,
                    'merged_count': 0,
                    'duration': (datetime.now() - start_time).total_seconds()
                }

            # 1. è·å–æŒ‡å®šçš„äº‹ä»¶
            events = await self.get_events_by_ids(event_ids)
            if not events or len(events) != len(event_ids):
                missing_ids = [eid for eid in event_ids if eid not in [e['id'] for e in events]]
                return {
                    'status': 'error',
                    'message': f'éƒ¨åˆ†äº‹ä»¶IDä¸å­˜åœ¨æˆ–çŠ¶æ€å¼‚å¸¸: {missing_ids}',
                    'total_events': len(events),
                    'suggestions_count': 0,
                    'merged_count': 0,
                    'duration': (datetime.now() - start_time).total_seconds()
                }

            # 2. åˆ›å»ºæ‰‹åŠ¨åˆå¹¶å»ºè®®
            primary_event_id = event_ids[0]
            primary_event = next(event for event in events if event['id'] == primary_event_id)

            merge_suggestion = self._create_manual_merge_suggestion(
                events, primary_event_id, primary_event
            )

            # 3. æ‰§è¡Œåˆå¹¶
            success = await self.execute_batch_merge(merge_suggestion)

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            if success:
                logger.info(f"æ‰‹åŠ¨äº‹ä»¶åˆå¹¶å®Œæˆ: åˆå¹¶äº‹ä»¶{event_ids}åˆ°ä¸»äº‹ä»¶{primary_event_id}, "
                          f"è€—æ—¶{duration:.2f}ç§’")

                return {
                    'status': 'success',
                    'message': f'æˆåŠŸåˆå¹¶äº‹ä»¶ {event_ids} åˆ°ä¸»äº‹ä»¶ {primary_event_id}',
                    'total_events': len(events),
                    'suggestions_count': 1,
                    'merged_count': 1,
                    'failed_count': 0,
                    'duration': duration
                }
            else:
                return {
                    'status': 'error',
                    'message': f'æ‰‹åŠ¨åˆå¹¶æ‰§è¡Œå¤±è´¥: {event_ids}',
                    'total_events': len(events),
                    'suggestions_count': 1,
                    'merged_count': 0,
                    'failed_count': 1,
                    'duration': duration
                }

        except Exception as e:
            logger.error(f"æ‰‹åŠ¨äº‹ä»¶åˆå¹¶æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': str(e),
                'total_events': 0,
                'suggestions_count': 0,
                'merged_count': 0,
                'duration': (datetime.now() - start_time).total_seconds()
            }

    async def run_combine_process(self) -> Dict:
        """
        è¿è¡Œäº‹ä»¶åˆå¹¶æµç¨‹ï¼ˆæ‰¹é‡åˆ†æç‰ˆï¼‰

        ä½¿ç”¨æ‰¹é‡LLMåˆ†ææ›¿ä»£é€å¯¹æ¯”è¾ƒï¼Œå¤§å¹…æå‡æ•ˆç‡

        Returns:
            åˆå¹¶ç»“æœç»Ÿè®¡
        """
        start_time = datetime.now()

        try:
            logger.info("å¼€å§‹æ‰§è¡Œäº‹ä»¶åˆå¹¶æµç¨‹ï¼ˆæ‰¹é‡åˆ†æç‰ˆï¼‰")

            # 1. è·å–æœ€è¿‘çš„äº‹ä»¶
            events = await self.get_recent_events()
            if len(events) < 2:
                return {
                    'status': 'success',
                    'message': 'äº‹ä»¶æ•°é‡ä¸è¶³ï¼Œæ— éœ€åˆå¹¶',
                    'total_events': len(events),
                    'suggestions_count': 0,
                    'merged_count': 0,
                    'duration': (datetime.now() - start_time).total_seconds()
                }

            # 2. æ‰¹é‡åˆ†æåˆå¹¶å»ºè®®
            merge_suggestions = await self.analyze_events_batch_merge(events)
            if not merge_suggestions:
                return {
                    'status': 'success',
                    'message': 'æœªå‘ç°éœ€è¦åˆå¹¶çš„äº‹ä»¶',
                    'total_events': len(events),
                    'suggestions_count': 0,
                    'merged_count': 0,
                    'duration': (datetime.now() - start_time).total_seconds()
                }

            # 3. æ‰§è¡Œæ‰€æœ‰æ‰¹é‡åˆå¹¶å»ºè®®
            merged_count = 0
            failed_merges = []

            # å¤„ç†æ‰€æœ‰åˆå¹¶å»ºè®®ï¼Œé¿å…å†²çªï¼ˆä¸€ä¸ªäº‹ä»¶åªèƒ½è¢«åˆå¹¶ä¸€æ¬¡ï¼‰
            processed_events = set()

            for suggestion in merge_suggestions:
                events_to_merge = suggestion['events_to_merge']

                # å¦‚æœä»»ä½•è¦åˆå¹¶çš„äº‹ä»¶å·²ç»è¢«å¤„ç†è¿‡ï¼Œè·³è¿‡
                if any(eid in processed_events for eid in events_to_merge):
                    logger.debug(f"è·³è¿‡å·²å¤„ç†çš„åˆå¹¶å»ºè®®: {events_to_merge}")
                    continue

                success = await self.execute_batch_merge(suggestion)
                if success:
                    merged_count += 1
                    processed_events.update(events_to_merge)
                else:
                    failed_merges.append({
                        'events_to_merge': events_to_merge,
                        'primary_event_id': suggestion['primary_event_id'],
                        'reason': 'æ‰§è¡Œæ‰¹é‡åˆå¹¶å¤±è´¥'
                    })

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            logger.info(f"æ‰¹é‡äº‹ä»¶åˆå¹¶æµç¨‹å®Œæˆ: åˆ†æäº†{len(events)}ä¸ªäº‹ä»¶, "
                      f"ç”Ÿæˆ{len(merge_suggestions)}ä¸ªåˆå¹¶ç»„, æˆåŠŸåˆå¹¶{merged_count}ä¸ªç»„, "
                      f"è€—æ—¶{duration:.2f}ç§’")

            return {
                'status': 'success',
                'message': f'æˆåŠŸåˆå¹¶{merged_count}ä¸ªäº‹ä»¶ç»„',
                'total_events': len(events),
                'suggestions_count': len(merge_suggestions),
                'merged_count': merged_count,
                'failed_count': len(failed_merges),
                'failed_merges': failed_merges,
                'duration': duration
            }

        except Exception as e:
            logger.error(f"æ‰¹é‡äº‹ä»¶åˆå¹¶æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': str(e),
                'total_events': 0,
                'suggestions_count': 0,
                'merged_count': 0,
                'duration': (datetime.now() - start_time).total_seconds()
            }


# å…¨å±€æœåŠ¡å®ä¾‹
event_combine_service = EventCombineService()