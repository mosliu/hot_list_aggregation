"""
äº‹ä»¶åˆå¹¶æœåŠ¡
è´Ÿè´£è¯†åˆ«å’Œåˆå¹¶ç›¸ä¼¼çš„çƒ­ç‚¹äº‹ä»¶
"""

import asyncio
import json
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from loguru import logger
from sqlalchemy.orm import Session
from sqlalchemy import and_, desc, or_

from database.connection import get_db_session
from models.hot_aggr_models import (
    HotAggrEvent,
    HotAggrNewsEventRelation,
    HotAggrEventHistoryRelation
)
from services.llm_wrapper import llm_wrapper
from services.prompt_templates import prompt_templates
from config.settings import settings


class EventCombineService:
    """äº‹ä»¶åˆå¹¶æœåŠ¡ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        self.combine_count = getattr(settings, 'EVENT_COMBINE_COUNT', 30)
        self.confidence_threshold = getattr(settings, 'EVENT_COMBINE_CONFIDENCE_THRESHOLD', 0.75)

    async def get_recent_events(self, count: int = None) -> List[Dict]:
        """
        è·å–æœ€è¿‘çš„äº‹ä»¶åˆ—è¡¨

        Args:
            count: è·å–çš„äº‹ä»¶æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼

        Returns:
            äº‹ä»¶åˆ—è¡¨
        """
        if count is None:
            count = self.combine_count

        try:
            with get_db_session() as db:
                events = db.query(HotAggrEvent).filter(
                    HotAggrEvent.status == 1  # åªè·å–æ­£å¸¸çŠ¶æ€çš„äº‹ä»¶
                ).order_by(
                    desc(HotAggrEvent.created_at)
                ).limit(count).all()

                event_list = []
                for event in events:
                    event_list.append({
                        'id': event.id,
                        'title': event.title or '',
                        'description': event.description or '',
                        'event_type': event.event_type or '',
                        'sentiment': event.sentiment or '',
                        'entities': event.entities or '',
                        'regions': event.regions or '',
                        'keywords': event.keywords or '',
                        'confidence_score': float(event.confidence_score or 0),
                        'news_count': event.news_count or 0,
                        'first_news_time': event.first_news_time,
                        'last_news_time': event.last_news_time,
                        'created_at': event.created_at,
                        'updated_at': event.updated_at
                    })

                logger.info(f"è·å–åˆ° {len(event_list)} ä¸ªæœ€è¿‘äº‹ä»¶")
                return event_list

        except Exception as e:
            logger.error(f"è·å–æœ€è¿‘äº‹ä»¶å¤±è´¥: {e}")
            raise

    def _format_events_for_batch_analysis(self, events: List[Dict]) -> str:
        """
        å°†äº‹ä»¶åˆ—è¡¨æ ¼å¼åŒ–ä¸ºæ‰¹é‡åˆ†æçš„JSONå­—ç¬¦ä¸²

        Args:
            events: äº‹ä»¶åˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–åçš„äº‹ä»¶JSONå­—ç¬¦ä¸²
        """
        try:
            formatted_events = []

            for event in events:
                formatted_event = {
                    'id': event['id'],
                    'title': event.get('title', ''),
                    'description': event.get('description', ''),
                    'event_type': event.get('event_type', ''),
                    'sentiment': event.get('sentiment', ''),
                    'entities': event.get('entities', ''),
                    'regions': event.get('regions', ''),
                    'keywords': event.get('keywords', ''),
                    'confidence_score': event.get('confidence_score', 0),
                    'news_count': event.get('news_count', 0),
                    'first_news_time': event.get('first_news_time').strftime('%Y-%m-%d %H:%M:%S') if event.get('first_news_time') else '',
                    'last_news_time': event.get('last_news_time').strftime('%Y-%m-%d %H:%M:%S') if event.get('last_news_time') else '',
                    'created_at': event.get('created_at').strftime('%Y-%m-%d %H:%M:%S') if event.get('created_at') else ''
                }
                formatted_events.append(formatted_event)

            # æ ¼å¼åŒ–ä¸ºå¯è¯»çš„JSONå­—ç¬¦ä¸²
            events_json = json.dumps(formatted_events, ensure_ascii=False, indent=2)
            return events_json

        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–äº‹ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return "[]"  # è¿”å›ç©ºæ•°ç»„ä½œä¸ºfallback

    async def analyze_events_batch_merge(self, events: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨LLMåˆ†æäº‹ä»¶å¯¹æ˜¯å¦åº”è¯¥åˆå¹¶ï¼ˆåŒ…å«é¢„ç­›é€‰ä¼˜åŒ–ï¼‰

        Args:
            events: äº‹ä»¶åˆ—è¡¨

        Returns:
            åˆå¹¶å»ºè®®åˆ—è¡¨
        """
        merge_suggestions = []

        try:
            total_pairs = len(events) * (len(events) - 1) // 2
            max_llm_calls = getattr(settings, 'EVENT_COMBINE_MAX_LLM_CALLS', 100)

            logger.info(f"ç†è®ºäº‹ä»¶å¯¹æ•°é‡: {total_pairs}")
            logger.info(f"æœ€å¤§LLMè°ƒç”¨æ¬¡æ•°: {max_llm_calls}")

            # é¢„ç­›é€‰è®¡æ•°
            analyzed_pairs = 0
            skipped_pairs = 0
            llm_calls_made = 0

            # ç”Ÿæˆäº‹ä»¶å¯¹è¿›è¡Œåˆ†æï¼ˆæ·»åŠ é¢„ç­›é€‰å’ŒLLMè°ƒç”¨é™åˆ¶ï¼‰
            logger.info(f"ğŸ” å¼€å§‹äº‹ä»¶å¯¹åˆ†æ...")
            for i in range(len(events)):
                if llm_calls_made >= max_llm_calls:
                    break
                for j in range(i + 1, len(events)):
                    event_a = events[i]
                    event_b = events[j]

                    # é¢„ç­›é€‰ï¼šè·³è¿‡æ˜æ˜¾ä¸éœ€è¦åˆ†æçš„äº‹ä»¶å¯¹
                    if not self._should_analyze_pair(event_a, event_b):
                        skipped_pairs += 1
                        logger.debug(f"â­ï¸ è·³è¿‡äº‹ä»¶å¯¹ {event_a['id']}-{event_b['id']} (é¢„ç­›é€‰æœªé€šè¿‡)")
                        continue

                    analyzed_pairs += 1

                    # æ£€æŸ¥LLMè°ƒç”¨æ¬¡æ•°é™åˆ¶
                    if llm_calls_made >= max_llm_calls:
                        logger.info(f"å·²è¾¾åˆ°æœ€å¤§LLMè°ƒç”¨æ¬¡æ•°é™åˆ¶ ({max_llm_calls})ï¼Œåœæ­¢åˆ†æ")
                        break

                    # ä½¿ç”¨ç°æœ‰çš„äº‹ä»¶åˆå¹¶å»ºè®®æ¨¡æ¿
                    prompt = prompt_templates.format_template(
                        'event_merge_suggestion',
                        event_a_id=event_a['id'],
                        event_a_title=event_a['title'],
                        event_a_summary=event_a['description'],
                        event_a_type=event_a['event_type'],
                        event_a_time=event_a['first_news_time'],
                        event_b_id=event_b['id'],
                        event_b_title=event_b['title'],
                        event_b_summary=event_b['description'],
                        event_b_type=event_b['event_type'],
                        event_b_time=event_b['first_news_time']
                    )

                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼ˆå¸¦é‡è¯•å’Œè¯¦ç»†æ—¥å¿—ï¼‰
                    try:
                        model_name = getattr(settings, 'EVENT_COMBINE_MODEL', 'gemini-2.5-pro')
                        temperature = getattr(settings, 'EVENT_COMBINE_TEMPERATURE', 0.7)
                        max_tokens = getattr(settings, 'EVENT_COMBINE_MAX_TOKENS', 2000)

                        # è®°å½•LLMè°ƒç”¨å¼€å§‹
                        logger.info(f"ğŸ” LLMè°ƒç”¨ #{llm_calls_made + 1}: åˆ†æäº‹ä»¶å¯¹ {event_a['id']}-{event_b['id']}")
                        logger.debug(f"  äº‹ä»¶A: {event_a['title'][:50]}...")
                        logger.debug(f"  äº‹ä»¶B: {event_b['title'][:50]}...")
                        logger.debug(f"  æ¨¡å‹: {model_name}, æ¸©åº¦: {temperature}")

                        # é‡è¯•æœºåˆ¶
                        max_retries = 3
                        response_text = None
                        call_start_time = datetime.now()

                        for retry in range(max_retries):
                            try:
                                retry_start_time = datetime.now()
                                logger.debug(f"  å°è¯• {retry + 1}/{max_retries}")

                                response_text = await llm_wrapper.call_llm_single(
                                    prompt=prompt,
                                    model=model_name,
                                    temperature=temperature,
                                    max_tokens=max_tokens
                                )

                                retry_duration = (datetime.now() - retry_start_time).total_seconds()

                                if response_text:
                                    logger.debug(f"  âœ… æˆåŠŸè·å–å“åº”ï¼Œè€—æ—¶: {retry_duration:.2f}ç§’")
                                    break
                                else:
                                    logger.warning(f"  âš ï¸ å“åº”ä¸ºç©ºï¼Œè€—æ—¶: {retry_duration:.2f}ç§’")

                            except Exception as retry_error:
                                retry_duration = (datetime.now() - retry_start_time).total_seconds()
                                if retry == max_retries - 1:
                                    logger.error(f"  âŒ æœ€ç»ˆå¤±è´¥ï¼Œè€—æ—¶: {retry_duration:.2f}ç§’, é”™è¯¯: {retry_error}")
                                    raise retry_error
                                logger.warning(f"  ğŸ”„ é‡è¯• {retry + 1}/{max_retries}ï¼Œè€—æ—¶: {retry_duration:.2f}ç§’, é”™è¯¯: {retry_error}")
                                await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•

                        # è®°å½•æ€»ä½“è°ƒç”¨ç»“æœ
                        total_duration = (datetime.now() - call_start_time).total_seconds()
                        llm_calls_made += 1
                        logger.info(f"ğŸ“Š LLMè°ƒç”¨ #{llm_calls_made} å®Œæˆï¼Œæ€»è€—æ—¶: {total_duration:.2f}ç§’")

                        # è§£æJSONå“åº”
                        if response_text:
                            try:
                                logger.debug(f"  ğŸ”§ å¼€å§‹è§£æJSONå“åº”ï¼ˆé•¿åº¦: {len(response_text)} å­—ç¬¦ï¼‰")
                                response = json.loads(response_text)
                                logger.debug(f"  âœ… JSONè§£ææˆåŠŸ")
                            except json.JSONDecodeError as json_error:
                                logger.warning(f"  âš ï¸ JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {json_error}")
                                # å°è¯•ä½¿ç”¨json_repairä¿®å¤JSON
                                import json_repair
                                try:
                                    response = json_repair.loads(response_text)
                                    logger.debug(f"  ğŸ”§ JSONä¿®å¤æˆåŠŸ")
                                except Exception as repair_error:
                                    logger.error(f"  âŒ JSONä¿®å¤å¤±è´¥: {repair_error}")
                                    logger.debug(f"  åŸå§‹å“åº”å‰200å­—ç¬¦: {response_text[:200]}...")
                                    response = None
                        else:
                            logger.warning(f"  âš ï¸ LLMå“åº”ä¸ºç©º")
                            response = None

                        # åˆ†æå“åº”ç»“æœ
                        if response and 'should_merge' in response:
                            should_merge = response.get('should_merge', False)
                            confidence = response.get('confidence', 0)
                            logger.debug(f"  ğŸ“ LLMåˆ†æç»“æœ: should_merge={should_merge}, confidence={confidence:.3f}")

                            # åªæœ‰å»ºè®®åˆå¹¶ä¸”ç½®ä¿¡åº¦è¶…è¿‡é˜ˆå€¼çš„æ‰åŠ å…¥ç»“æœ
                            if should_merge and confidence >= self.confidence_threshold:
                                merge_suggestion = {
                                    'source_event_id': event_b['id'],  # è¾ƒæ–°çš„äº‹ä»¶ä½œä¸ºæºäº‹ä»¶
                                    'target_event_id': event_a['id'],  # è¾ƒæ—©çš„äº‹ä»¶ä½œä¸ºç›®æ ‡äº‹ä»¶
                                    'source_event': event_b,
                                    'target_event': event_a,
                                    'confidence': confidence,
                                    'reason': response.get('reason', ''),
                                    'merged_title': response.get('merged_title', ''),
                                    'merged_summary': response.get('merged_summary', ''),
                                    'analysis': response.get('analysis', {})
                                }
                                merge_suggestions.append(merge_suggestion)

                                logger.info(f"ğŸ¯ å‘ç°åˆå¹¶å»ºè®®: äº‹ä»¶{event_b['id']} -> äº‹ä»¶{event_a['id']}, "
                                          f"ç½®ä¿¡åº¦: {confidence:.3f}, åŸå› : {response.get('reason', '')[:50]}...")
                            else:
                                if should_merge:
                                    logger.debug(f"  âŒ åˆå¹¶å»ºè®®ç½®ä¿¡åº¦ä¸è¶³: {confidence:.3f} < {self.confidence_threshold}")
                                else:
                                    logger.debug(f"  âŒ LLMåˆ¤æ–­ä¸éœ€è¦åˆå¹¶")
                        else:
                            logger.warning(f"  âš ï¸ LLMå“åº”æ ¼å¼æ— æ•ˆæˆ–ç¼ºå°‘å…³é”®å­—æ®µ")

                    except Exception as e:
                        logger.error(f"âŒ åˆ†æäº‹ä»¶å¯¹ {event_a['id']}-{event_b['id']} å®Œå…¨å¤±è´¥: {e}")
                        continue

            # æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
            merge_suggestions.sort(key=lambda x: x['confidence'], reverse=True)

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            logger.info(f"äº‹ä»¶å¯¹åˆ†æç»Ÿè®¡:")
            logger.info(f"  æ€»äº‹ä»¶å¯¹æ•°: {total_pairs}")
            logger.info(f"  é¢„ç­›é€‰è·³è¿‡: {skipped_pairs}")
            logger.info(f"  å®é™…åˆ†æ: {analyzed_pairs}")
            logger.info(f"  LLMè°ƒç”¨æ¬¡æ•°: {llm_calls_made}")
            logger.info(f"  ç­›é€‰æ•ˆç‡: {(skipped_pairs/total_pairs*100):.1f}%")
            logger.info(f"  åˆå¹¶å»ºè®®: {len(merge_suggestions)} ä¸ª")

            return merge_suggestions

        except Exception as e:
            logger.error(f"åˆ†æäº‹ä»¶åˆå¹¶å¤±è´¥: {e}")
            raise

    def _merge_event_data(self, source_event: Dict, target_event: Dict,
                         merged_title: str, merged_summary: str) -> Dict:
        """
        èåˆä¸¤ä¸ªäº‹ä»¶çš„æ•°æ®

        Args:
            source_event: æºäº‹ä»¶ï¼ˆå°†è¢«åˆå¹¶ï¼‰
            target_event: ç›®æ ‡äº‹ä»¶ï¼ˆä¿ç•™ï¼‰
            merged_title: åˆå¹¶åçš„æ ‡é¢˜
            merged_summary: åˆå¹¶åçš„æ‘˜è¦

        Returns:
            èåˆåçš„äº‹ä»¶æ•°æ®
        """
        try:
            # åˆå¹¶regions
            source_regions = set()
            target_regions = set()

            if source_event.get('regions'):
                regions_str = source_event['regions']
                if regions_str.startswith('['):
                    try:
                        source_regions.update(json.loads(regions_str))
                    except:
                        source_regions.update([r.strip() for r in regions_str.split(',') if r.strip()])
                else:
                    source_regions.update([r.strip() for r in regions_str.split(',') if r.strip()])

            if target_event.get('regions'):
                regions_str = target_event['regions']
                if regions_str.startswith('['):
                    try:
                        target_regions.update(json.loads(regions_str))
                    except:
                        target_regions.update([r.strip() for r in regions_str.split(',') if r.strip()])
                else:
                    target_regions.update([r.strip() for r in regions_str.split(',') if r.strip()])

            merged_regions = ','.join(sorted(source_regions | target_regions))

            # åˆå¹¶keywords
            source_keywords = set()
            target_keywords = set()

            if source_event.get('keywords'):
                source_keywords.update([k.strip() for k in source_event['keywords'].split(',') if k.strip()])
            if target_event.get('keywords'):
                target_keywords.update([k.strip() for k in target_event['keywords'].split(',') if k.strip()])

            merged_keywords = ','.join(sorted(source_keywords | target_keywords))

            # åˆå¹¶entitiesï¼ˆå¦‚æœæ˜¯JSONæ ¼å¼ï¼‰
            merged_entities = target_event.get('entities', '') or source_event.get('entities', '')

            # å–è¾ƒæ—©çš„é¦–æ¬¡æŠ¥é“æ—¶é—´å’Œè¾ƒæ™šçš„æœ€åæ›´æ–°æ—¶é—´
            first_news_time = min(
                source_event.get('first_news_time') or datetime.max,
                target_event.get('first_news_time') or datetime.max
            )
            last_news_time = max(
                source_event.get('last_news_time') or datetime.min,
                target_event.get('last_news_time') or datetime.min
            )

            return {
                'title': merged_title or target_event['title'],
                'description': merged_summary or target_event['description'],
                'event_type': target_event.get('event_type') or source_event.get('event_type'),
                'sentiment': target_event.get('sentiment') or source_event.get('sentiment'),
                'entities': merged_entities,
                'regions': merged_regions,
                'keywords': merged_keywords,
                'first_news_time': first_news_time if first_news_time != datetime.max else None,
                'last_news_time': last_news_time if last_news_time != datetime.min else None,
                'news_count': (source_event.get('news_count', 0) + target_event.get('news_count', 0))
            }

        except Exception as e:
            logger.error(f"èåˆäº‹ä»¶æ•°æ®å¤±è´¥: {e}")
            return {
                'title': merged_title or target_event['title'],
                'description': merged_summary or target_event['description'],
                'regions': target_event.get('regions', ''),
                'keywords': target_event.get('keywords', ''),
                'news_count': target_event.get('news_count', 0)
            }

    async def execute_merge(self, merge_suggestion: Dict) -> bool:
        """
        æ‰§è¡Œäº‹ä»¶åˆå¹¶

        Args:
            merge_suggestion: åˆå¹¶å»ºè®®

        Returns:
            æ˜¯å¦åˆå¹¶æˆåŠŸ
        """
        source_event_id = merge_suggestion['source_event_id']
        target_event_id = merge_suggestion['target_event_id']

        try:
            with get_db_session() as db:
                # å¼€å§‹æ•°æ®åº“äº‹åŠ¡
                # 1. è·å–æºäº‹ä»¶å’Œç›®æ ‡äº‹ä»¶
                source_event = db.query(HotAggrEvent).filter(
                    HotAggrEvent.id == source_event_id
                ).first()
                target_event = db.query(HotAggrEvent).filter(
                    HotAggrEvent.id == target_event_id
                ).first()

                if not source_event or not target_event:
                    logger.error(f"äº‹ä»¶ä¸å­˜åœ¨: æºäº‹ä»¶{source_event_id}, ç›®æ ‡äº‹ä»¶{target_event_id}")
                    return False

                # æ£€æŸ¥äº‹ä»¶çŠ¶æ€
                if source_event.status != 1 or target_event.status != 1:
                    logger.warning(f"äº‹ä»¶çŠ¶æ€å¼‚å¸¸: æºäº‹ä»¶{source_event.status}, ç›®æ ‡äº‹ä»¶{target_event.status}")
                    return False

                # 2. èåˆäº‹ä»¶æ•°æ®
                merged_data = self._merge_event_data(
                    merge_suggestion['source_event'],
                    merge_suggestion['target_event'],
                    merge_suggestion['merged_title'],
                    merge_suggestion['merged_summary']
                )

                # 3. æ›´æ–°ç›®æ ‡äº‹ä»¶
                target_event.title = merged_data['title']
                target_event.description = merged_data['description']
                target_event.event_type = merged_data.get('event_type')
                target_event.sentiment = merged_data.get('sentiment')
                target_event.entities = merged_data.get('entities')
                target_event.regions = merged_data['regions']
                target_event.keywords = merged_data['keywords']
                target_event.first_news_time = merged_data.get('first_news_time')
                target_event.last_news_time = merged_data.get('last_news_time')
                target_event.news_count = merged_data['news_count']
                target_event.updated_at = datetime.now()

                # 4. æ›´æ–°æºäº‹ä»¶çŠ¶æ€ä¸ºå·²åˆå¹¶
                source_event.status = 2  # å·²åˆå¹¶çŠ¶æ€
                source_event.updated_at = datetime.now()

                # 5. è½¬ç§»æ–°é—»å…³è”å…³ç³»
                news_relations = db.query(HotAggrNewsEventRelation).filter(
                    HotAggrNewsEventRelation.event_id == source_event_id
                ).all()

                transferred_news_count = 0
                for relation in news_relations:
                    # æ£€æŸ¥ç›®æ ‡äº‹ä»¶æ˜¯å¦å·²ç»æœ‰è¿™æ¡æ–°é—»çš„å…³è”
                    existing_relation = db.query(HotAggrNewsEventRelation).filter(
                        and_(
                            HotAggrNewsEventRelation.news_id == relation.news_id,
                            HotAggrNewsEventRelation.event_id == target_event_id
                        )
                    ).first()

                    if existing_relation:
                        # å¦‚æœå·²å­˜åœ¨ï¼Œåˆ é™¤æºäº‹ä»¶çš„å…³è”
                        db.delete(relation)
                    else:
                        # è½¬ç§»åˆ°ç›®æ ‡äº‹ä»¶
                        relation.event_id = target_event_id
                        transferred_news_count += 1

                # 6. è®°å½•åˆå¹¶å†å²å…³ç³»
                history_relation = HotAggrEventHistoryRelation(
                    parent_event_id=target_event_id,
                    child_event_id=source_event_id,
                    relation_type='merge',
                    confidence_score=merge_suggestion['confidence'],
                    description=f"äº‹ä»¶åˆå¹¶: {merge_suggestion['reason']}",
                    created_at=datetime.now()
                )
                db.add(history_relation)

                # æäº¤äº‹åŠ¡
                db.commit()

                logger.info(f"æˆåŠŸåˆå¹¶äº‹ä»¶: æºäº‹ä»¶{source_event_id} -> ç›®æ ‡äº‹ä»¶{target_event_id}, "
                          f"è½¬ç§»æ–°é—»å…³è” {transferred_news_count} æ¡, ç½®ä¿¡åº¦: {merge_suggestion['confidence']:.3f}")
                return True

        except Exception as e:
            logger.error(f"æ‰§è¡Œäº‹ä»¶åˆå¹¶å¤±è´¥: {e}")
            try:
                db.rollback()
            except:
                pass
            return False

    async def run_combine_process(self) -> Dict:
        """
        è¿è¡Œäº‹ä»¶åˆå¹¶æµç¨‹

        åˆ†ææ‰€æœ‰äº‹ä»¶å¯¹ï¼Œå‘ç°åˆå¹¶å»ºè®®å°±æ‰§è¡Œï¼Œæ²¡æœ‰å°±å¿½ç•¥

        Returns:
            åˆå¹¶ç»“æœç»Ÿè®¡
        """
        start_time = datetime.now()

        try:
            logger.info("å¼€å§‹æ‰§è¡Œäº‹ä»¶åˆå¹¶æµç¨‹")

            # 1. è·å–æœ€è¿‘çš„äº‹ä»¶
            events = await self.get_recent_events()
            if len(events) < 2:
                return {
                    'status': 'success',
                    'message': 'äº‹ä»¶æ•°é‡ä¸è¶³ï¼Œæ— éœ€åˆå¹¶',
                    'total_events': len(events),
                    'suggestions_count': 0,
                    'merged_count': 0,
                    'duration': (datetime.now() - start_time).total_seconds()
                }

            # 2. åˆ†æåˆå¹¶å»ºè®®
            merge_suggestions = await self.analyze_event_pairs_for_merge(events)
            if not merge_suggestions:
                return {
                    'status': 'success',
                    'message': 'æœªå‘ç°éœ€è¦åˆå¹¶çš„äº‹ä»¶',
                    'total_events': len(events),
                    'suggestions_count': 0,
                    'merged_count': 0,
                    'duration': (datetime.now() - start_time).total_seconds()
                }

            # 3. æ‰§è¡Œæ‰€æœ‰åˆå¹¶å»ºè®®
            merged_count = 0
            failed_merges = []

            # å¤„ç†æ‰€æœ‰åˆå¹¶å»ºè®®ï¼Œä½†è¦é¿å…å¾ªç¯åˆå¹¶ï¼ˆä¸€ä¸ªäº‹ä»¶è¢«å¤šæ¬¡åˆå¹¶ï¼‰
            processed_events = set()

            for suggestion in merge_suggestions:
                source_id = suggestion['source_event_id']
                target_id = suggestion['target_event_id']

                # å¦‚æœæºäº‹ä»¶æˆ–ç›®æ ‡äº‹ä»¶å·²ç»è¢«å¤„ç†è¿‡ï¼Œè·³è¿‡
                if source_id in processed_events or target_id in processed_events:
                    continue

                success = await self.execute_merge(suggestion)
                if success:
                    merged_count += 1
                    processed_events.add(source_id)
                    # æ³¨æ„ï¼šç›®æ ‡äº‹ä»¶å¯ä»¥ç»§ç»­ä½œä¸ºå…¶ä»–åˆå¹¶çš„ç›®æ ‡
                else:
                    failed_merges.append({
                        'source_event_id': source_id,
                        'target_event_id': target_id,
                        'reason': 'æ‰§è¡Œåˆå¹¶å¤±è´¥'
                    })

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            logger.info(f"äº‹ä»¶åˆå¹¶æµç¨‹å®Œæˆ: åˆ†æäº†{len(events)}ä¸ªäº‹ä»¶, "
                      f"ç”Ÿæˆ{len(merge_suggestions)}ä¸ªå»ºè®®, æˆåŠŸåˆå¹¶{merged_count}ä¸ªäº‹ä»¶, "
                      f"è€—æ—¶{duration:.2f}ç§’")

            return {
                'status': 'success',
                'message': f'æˆåŠŸåˆå¹¶{merged_count}ä¸ªäº‹ä»¶',
                'total_events': len(events),
                'suggestions_count': len(merge_suggestions),
                'merged_count': merged_count,
                'failed_count': len(failed_merges),
                'failed_merges': failed_merges,
                'duration': duration
            }

        except Exception as e:
            logger.error(f"äº‹ä»¶åˆå¹¶æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': str(e),
                'total_events': 0,
                'suggestions_count': 0,
                'merged_count': 0,
                'duration': (datetime.now() - start_time).total_seconds()
            }


# å…¨å±€æœåŠ¡å®ä¾‹
event_combine_service = EventCombineService()